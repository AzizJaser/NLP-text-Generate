{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0K2uTw6VTJTaEqbbFaqYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzizJaser/NLP-text-Generate/blob/main/Generate_text_according_to_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer() # convert text to sequences of integers, where each integer represents a specific word\n",
        "\n",
        "with open('/content/fa6.txt', 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "\n",
        "\n",
        "corpus = data.lower().split(\"\\n\") #The text data is converted to lowercase and split into lines. Each line is considered a separate entry in the corpus.\n",
        "\n",
        "tokenizer.fit_on_texts(corpus) # tokenizer processes the corpus and assigns a unique integer ID to each distinct word in the corpus.\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_sequences = [] # Initialize an empty list to store input sequences\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0] # Convert each line to a sequence of tokens integers\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1] # Create n-gram sequences for each line\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences]) # Find the length of the longest sequence\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # Pad all sequences to the length of the longest sequence\n",
        "\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1] # Separate the sequences into input data xs and labels last word of each sequence\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) # One-hot encode the labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential() # Build the model\n",
        "\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence_len-1)) # Add an Embedding layer (size of vocab,dimensionality of the embedding vectors,length of input)\n",
        "model.add(Bidirectional(LSTM(150))) # Add a Bidirectional LSTM layer (numbers of units)\n",
        "model.add(Dense(total_words, activation='softmax')) # Add a Dense layer with softmax activation for output (numbers of dimensionality optput, the activation fun)\n",
        "adam = Adam(lr=0.01) # Initialize the Adam optimizer (ratio)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) # Compile the model\n",
        "history = model.fit(xs, ys, epochs=30, verbose=1) # Train the model\n",
        "\n",
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC9p2C_xwNYT",
        "outputId": "a3912d29-bb2e-44ee-bda4-fa9ebaf2bfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "2252/2252 [==============================] - 175s 73ms/step - loss: 7.5638 - accuracy: 0.1114\n",
            "Epoch 2/30\n",
            "2252/2252 [==============================] - 81s 36ms/step - loss: 6.5541 - accuracy: 0.1614\n",
            "Epoch 3/30\n",
            "2252/2252 [==============================] - 75s 33ms/step - loss: 5.8792 - accuracy: 0.1827\n",
            "Epoch 4/30\n",
            "2252/2252 [==============================] - 74s 33ms/step - loss: 5.2310 - accuracy: 0.2028\n",
            "Epoch 5/30\n",
            "2252/2252 [==============================] - 71s 31ms/step - loss: 4.6103 - accuracy: 0.2301\n",
            "Epoch 6/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 4.0331 - accuracy: 0.2753\n",
            "Epoch 7/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 3.5090 - accuracy: 0.3457\n",
            "Epoch 8/30\n",
            "2252/2252 [==============================] - 71s 31ms/step - loss: 3.0355 - accuracy: 0.4247\n",
            "Epoch 9/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 2.6241 - accuracy: 0.4918\n",
            "Epoch 10/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 2.2762 - accuracy: 0.5525\n",
            "Epoch 11/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 1.9824 - accuracy: 0.6057\n",
            "Epoch 12/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 1.7364 - accuracy: 0.6495\n",
            "Epoch 13/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 1.5269 - accuracy: 0.6904\n",
            "Epoch 14/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 1.3446 - accuracy: 0.7258\n",
            "Epoch 15/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 1.1876 - accuracy: 0.7575\n",
            "Epoch 16/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 1.0541 - accuracy: 0.7838\n",
            "Epoch 17/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 0.9357 - accuracy: 0.8075\n",
            "Epoch 18/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.8306 - accuracy: 0.8290\n",
            "Epoch 19/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.7411 - accuracy: 0.8477\n",
            "Epoch 20/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.6615 - accuracy: 0.8648\n",
            "Epoch 21/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.5947 - accuracy: 0.8783\n",
            "Epoch 22/30\n",
            "2252/2252 [==============================] - 69s 30ms/step - loss: 0.5370 - accuracy: 0.8902\n",
            "Epoch 23/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.4879 - accuracy: 0.9001\n",
            "Epoch 24/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.4434 - accuracy: 0.9091\n",
            "Epoch 25/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.4090 - accuracy: 0.9159\n",
            "Epoch 26/30\n",
            "2252/2252 [==============================] - 69s 31ms/step - loss: 0.3767 - accuracy: 0.9215\n",
            "Epoch 27/30\n",
            "2252/2252 [==============================] - 70s 31ms/step - loss: 0.3506 - accuracy: 0.9261\n",
            "Epoch 28/30\n",
            "2252/2252 [==============================] - 68s 30ms/step - loss: 0.3257 - accuracy: 0.9315\n",
            "Epoch 29/30\n",
            "2252/2252 [==============================] - 68s 30ms/step - loss: 0.3075 - accuracy: 0.9343\n",
            "Epoch 30/30\n",
            "2252/2252 [==============================] - 68s 30ms/step - loss: 0.2937 - accuracy: 0.9373\n",
            "<keras.src.engine.sequential.Sequential object at 0x7cba334b5150>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_text = \"عثمان\" # Starting text for the sequence\n",
        "next_words = 25 # Number of words to generate\n",
        "\n",
        "for _ in range(next_words): # Loop to generate the specified number of words\n",
        "\ttoken_list = tokenizer.texts_to_sequences([start_text])[0] # Convert the current sequence of words to a list of token IDs\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # Pad the sequence to ensure it has a consistent length\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1) # Use the model to predict the next word as a token ID\n",
        "\toutput = \"\"  # Initialize a variable to hold the predicted word\n",
        "\tfor word, index in tokenizer.word_index.items(): # Loop through the word index to find the word corresponding to the predicted token ID\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput = word\n",
        "\t\t\tbreak\n",
        "\tstart_text += \" \" + output # Add the predicted word to the current sequence\n",
        "print(start_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0817btU0Trt",
        "outputId": "ca2e1331-fc6f-4b50-e6d0-bc4c9f1b6981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "عثمان بن عفان معه إمرأته رقية ابنة رسول الله صلى الله عليه وسلم، وأبو حذيفة بن عتبة بن عبد الله بن عمر بن مخزوم، فولدت له\n"
          ]
        }
      ]
    }
  ]
}